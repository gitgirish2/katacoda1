Welcome to Scenario 1 of the "Serving a Machine Learning Model in a Private Cloud" Tutorial.

The goal of this tutorial is to provide a basic example of how to set up an environment, create a model and serve a model in order to integrate it into an application.  

At the end of this tutorial, you should be able to call an endpoint to your model from your application, have it take in data, save the data to make the scenario evergreen and return a prediction to the caller.

<pre>
![ML Integration Flow](/lauraschornack/katacoda1/scenarios/set-up/assets/ML-Model-App-Integration.png)
</pre>

<pre>
![ML Integration Flow](/katacoda1/scenarios/set-up/assets/ML-Model-App-Integration.png)
</pre>

<pre>
![ML Integration Flow](/katacoda1/scenarios/set-up/assets/ML-Model-App-Integration.png)
</pre>


This tutorial has three scenarios.

Scenario 1:
Environment Set-up

Scenario 2:
Creating a simple ML model

Scenario 3:
Serve your ML model in Flask, giving your application an endpoint to call.

In the first scenario, we will concentrate on environment setup,
This scenario will take us through the steps we need to set up our 
tensorflow and python environment to create a basic ML model.
If you choose to do containers, go with Option 1.
If you chose to do a setup without containers, skip to Option 2.  
You do not need both options. 

The container approach is recommended for this tutorial.


